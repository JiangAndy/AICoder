# å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•

## åŠ¨æœº
åŸºäº Transformers æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¦‚ GPTã€T5 å’Œ BERTï¼Œå·²ç»åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜å¼€å§‹æ¶‰è¶³å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰ (CV) (VITã€Stable Diffusionã€LayoutLM) å’ŒéŸ³é¢‘ (Whisperã€XLS-R)ã€‚ä¼ ç»Ÿçš„èŒƒå¼æ˜¯å¯¹é€šç”¨ç½‘ç»œè§„æ¨¡æ•°æ®è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç„¶åå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸ä½¿ç”¨å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒ LLM (ä¾‹å¦‚ï¼Œé›¶æ ·æœ¬æ¨ç†) ç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›é¢„è®­ç»ƒ LLM ä¼šå¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ã€‚

ç„¶è€Œï¼Œéšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå…¨éƒ¨å‚æ•°çš„å¾®è°ƒå˜å¾—ä¸å¯è¡Œã€‚æ­¤å¤–ï¼Œä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ç‹¬ç«‹å­˜å‚¨å’Œéƒ¨ç½²å¾®è°ƒæ¨¡å‹å˜å¾—éå¸¸æ˜‚è´µï¼Œå› ä¸ºå¾®è°ƒæ¨¡å‹ä¸åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„å¤§å°ç›¸åŒã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼

PEFT æ–¹æ³•ä»…å¾®è°ƒå°‘é‡ (é¢å¤–) æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶å†»ç»“é¢„è®­ç»ƒ LLM çš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚è¿™ä¹Ÿå…‹æœäº†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ LLM çš„å…¨å‚æ•°å¾®è°ƒæœŸé—´è§‚å¯Ÿåˆ°çš„ä¸€ç§ç°è±¡ã€‚PEFT æ–¹æ³•ä¹Ÿæ˜¾ç¤ºå‡ºåœ¨ä½æ•°æ®çŠ¶æ€ä¸‹æ¯”å¾®è°ƒæ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ä»¥åŠ Stable diffusion dreamboothã€‚

PEFT æ–¹æ³•è¿˜æœ‰åŠ©äºæé«˜è½»ä¾¿æ€§ï¼Œå…¶ä¸­ç”¨æˆ·å¯ä»¥ä½¿ç”¨ PEFT æ–¹æ³•è°ƒæ•´æ¨¡å‹ï¼Œä»¥è·å¾—ä¸å®Œå…¨å¾®è°ƒçš„å¤§å‹æ£€æŸ¥ç‚¹ç›¸æ¯”ï¼Œå¤§å°ä»…å‡  MB çš„å¾®å°æ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œ bigscience/mt0-xxl å ç”¨ 40GB çš„å­˜å‚¨ç©ºé—´ï¼Œå…¨å‚æ•°å¾®è°ƒå°†å¯¼è‡´æ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†æœ‰å¯¹åº” 40GB æ£€æŸ¥ç‚¹ã€‚è€Œä½¿ç”¨ PEFT æ–¹æ³•ï¼Œæ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†åªå ç”¨å‡  MB çš„å­˜å‚¨ç©ºé—´ï¼ŒåŒæ—¶å®ç°ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚æ¥è‡ª PEFT æ–¹æ³•çš„å°‘é‡è®­ç»ƒæƒé‡è¢«æ·»åŠ åˆ°é¢„è®­ç»ƒ LLM é¡¶å±‚ã€‚å› æ­¤ï¼ŒåŒä¸€ä¸ª LLM å¯ä»¥é€šè¿‡æ·»åŠ å°çš„æƒé‡æ¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œè€Œæ— éœ€æ›¿æ¢æ•´ä¸ªæ¨¡å‹ã€‚

ç®€è€Œè¨€ä¹‹ï¼ŒPEFT æ–¹æ³•ä½¿æ‚¨èƒ½å¤Ÿè·å¾—ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åªæœ‰å°‘é‡å¯è®­ç»ƒå‚æ•°ã€‚

ğŸ¤— PEFT åº“æä¾›äº†æœ€æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œä¸ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerate æ— ç¼é›†æˆã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä½¿ç”¨æ¥è‡ª Transformers çš„æœ€æµè¡Œå’Œé«˜æ€§èƒ½çš„æ¨¡å‹ï¼Œä»¥åŠ Accelerate çš„ç®€å•æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä»¥ä¸‹æ˜¯ç›®å‰æ”¯æŒçš„ PEFT æ–¹æ³•:

LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
P-Tuning: [GPT Understands](https://arxiv.org/pdf/2103.10385.pdf), [Too](https://arxiv.org/pdf/2103.10385.pdf)

## ç¯å¢ƒå‡†å¤‡
é¦–å…ˆå®‰è£… ğŸ¤— PEFTï¼š
```shell
pip install peft
```

å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„ç‰¹æ€§ï¼Œä½ å¯èƒ½ä¼šæœ‰å…´è¶£ä»æºä»£ç å®‰è£…è¿™ä¸ªåº“ï¼š

```shell
pip install git+https://github.com/huggingface/peft.git
```

## ä½¿ç”¨ ğŸ¤— PEFT è®­ç»ƒæ‚¨çš„æ¨¡å‹
### å¼•è¿›å¿…è¦çš„åº“
```python
  from transformers import AutoModelForSeq2SeqLM
+ from peft import get_peft_model, LoraConfig, TaskType
  model_name_or_path = "bigscience/mt0-large"
  tokenizer_name_or_path = "bigscience/mt0-large"
```

### åˆ›å»º PEFT æ–¹æ³•å¯¹åº”çš„é…ç½®
```python
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
```

### é€šè¿‡è°ƒç”¨ get_peft_model åŒ…è£…åŸºç¡€ ğŸ¤— Transformer æ¨¡å‹
```python
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get_peft_model(model, peft_config)
+ model.print_trainable_parameters()
# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282
```
è®­ç»ƒå¾ªç¯çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ã€‚

### å½“æ‚¨å‡†å¤‡å¥½ä¿å­˜æ¨¡å‹ä»¥ä¾›æ¨ç†æ—¶ï¼Œåªéœ€æ‰§è¡Œä»¥ä¸‹æ“ä½œã€‚
```python
model.save_pretrained("output_dir") 
# model.push_to_hub("my_awesome_peft_model") also works
```
è¿™åªä¼šä¿å­˜ç»è¿‡è®­ç»ƒçš„å¢é‡ PEFT æƒé‡ã€‚

### è¦åŠ è½½å®ƒè¿›è¡Œæ¨ç†ï¼Œè¯·éµå¾ªä»¥ä¸‹ä»£ç ç‰‡æ®µ:
```python
  from transformers import AutoModelForSeq2SeqLM
+ from peft import PeftModel, PeftConfig

  peft_model_id = "smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM"
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
+ model = PeftModel.from_pretrained(model, peft_model_id)
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to(device)
  model.eval()
  inputs = tokenizer("Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :", return_tensors="pt")

  with torch.no_grad():
      outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=10)
      print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
# 'complaint'
```
