# å¤§æ¨¡å‹è®­ç»ƒå·¥å…·ä¹‹Accelerate

# accelerateåŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒ

éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå¹¶è¡Œæ€§å·²ç»æˆä¸ºåœ¨æœ‰é™ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§æ¨¡å‹å’ŒåŠ é€Ÿè®­ç»ƒé€Ÿåº¦çš„ç­–ç•¥ï¼Œå¢åŠ äº†æ•°ä¸ªæ•°é‡çº§ã€‚Hugging Faceï¼Œæä¾›äº†ğŸ¤— [åŠ é€Ÿåº“](https://huggingface.co/docs/accelerate)ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè½»æ¾è®­ç»ƒğŸ¤— Transformersæ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šçš„å¤šä¸ªGPUè¿˜æ˜¯åœ¨å¤šä¸ªæœºå™¨ä¸Šçš„å¤šä¸ªGPUã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œäº†è§£å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„åŸç”ŸPyTorchè®­ç»ƒå¾ªç¯ï¼Œä»¥å¯ç”¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è®­ç»ƒã€‚

## è®¾ç½®
é€šè¿‡å®‰è£…ğŸ¤— åŠ é€Ÿå¼€å§‹:

```shell
pip install accelerate
```

ç„¶åå¯¼å…¥å¹¶åˆ›å»º`Accelerator`å¯¹è±¡ã€‚`Accelerator`å°†è‡ªåŠ¨æ£€æµ‹æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ç±»å‹ï¼Œå¹¶åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶ã€‚æ‚¨ä¸éœ€è¦æ˜¾å¼åœ°å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šã€‚

```python
from accelerate import Accelerator

accelerator = Accelerator()
```

## å‡†å¤‡åŠ é€Ÿ
ä¸‹ä¸€æ­¥æ˜¯å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™`prepare`æ–¹æ³•ã€‚è¿™åŒ…æ‹¬æ‚¨çš„è®­ç»ƒå’Œè¯„ä¼°DataLoaderã€ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªä¼˜åŒ–å™¨:

```python
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

## åå‘ä¼ æ’­
æœ€åä¸€æ­¥æ˜¯ç”¨ğŸ¤— åŠ é€Ÿçš„`backward`æ–¹æ³•æ›¿æ¢è®­ç»ƒå¾ªç¯ä¸­çš„å…¸å‹`loss.backward()`:

```python
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

å¦‚æ‚¨åœ¨ä¸‹é¢çš„ä»£ç ä¸­æ‰€è§ï¼Œæ‚¨åªéœ€è¦æ·»åŠ å››è¡Œé¢å¤–çš„ä»£ç åˆ°æ‚¨çš„è®­ç»ƒå¾ªç¯ä¸­å³å¯å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼

```python
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## å¯åŠ¨è®­ç»ƒ
è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå’Œä¿å­˜é…ç½®æ–‡ä»¶:
```shell
accelerate config
```

ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ:
```shell
accelerate launch train.py
```

## è®­ç»ƒfeature
Accelerate æä¾›äº†é¢å¤–çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ¢¯åº¦ç´¯ç§¯ (gradient accumulation)ã€æ¢¯åº¦è£å‰ª (gradient clipping)ã€æ··åˆç²¾åº¦è®­ç»ƒ (mixed precision training)ç­‰ï¼Œæ‚¨å¯ä»¥å°†å…¶æ·»åŠ åˆ°è„šæœ¬ä¸­ä»¥æ”¹è¿›è®­ç»ƒã€‚

### æ¢¯åº¦ç´¯ç§¯
æ¢¯åº¦ç´¯ç§¯ä½¿æ‚¨èƒ½å¤Ÿåœ¨æ›´æ–°æƒé‡ä¹‹å‰é€šè¿‡ç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦æ¥è·å–æ›´å¤§çš„ç­‰æ•ˆ `batch_size`ã€‚è¿™å¯¹äºè§£å†³æ˜¾å­˜å¯¹ `batch_size` çš„é™åˆ¶å¾ˆæœ‰ç”¨ã€‚

è¦åœ¨ Accelerate ä¸­å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨åŠ é€Ÿå™¨ç±»ä¸­æŒ‡å®š `gradient_accumulation_steps` å‚æ•°ï¼Œå¹¶åœ¨è„šæœ¬ä¸­æ·»åŠ  `accumulate()` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š

```python
+ accelerator = Accelerator(gradient_accumulation_steps=2)
  model, optimizer, training_dataloader = accelerator.prepare(
      model, optimizer, training_dataloader
  )

  for input, label in training_dataloader:
+     with accelerator.accumulate(model):
          predictions = model(input)
          loss = loss_function(predictions, label)
          accelerator.backward(loss)
          optimizer.step()
          scheduler.step()
          optimizer.zero_grad()

```

### æ¢¯åº¦è£å‰ª
æ¢¯åº¦è£å‰ªæ˜¯ä¸€ç§é˜²æ­¢â€œæ¢¯åº¦çˆ†ç‚¸â€çš„æŠ€æœ¯ï¼ŒAccelerate æä¾›ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š

- `clip_grad_value_`ï¼šå°†å¯è¿­ä»£å‚æ•°çš„æ¢¯åº¦è£å‰ªä¸ºæŒ‡å®šå€¼ã€‚ æ¢¯åº¦å°±åœ°ä¿®æ”¹ï¼ˆin-placeï¼‰ã€‚
  - `parametres`ï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–
  - `clip_value`ï¼šæ¢¯åº¦çš„é˜ˆå€¼ã€‚æ¢¯åº¦è¢«é™åˆ¶åœ¨èŒƒå›´å†…
- `clip_grad_norm_`ï¼šèŒƒæ•°æ˜¯å¯¹æ‰€æœ‰æ¢¯åº¦ä¸€èµ·è®¡ç®—çš„ã€‚æ¢¯åº¦å°±åœ°ä¿®æ”¹ã€‚
  - `parameters`ï¼šå¯è¿­ä»£çš„å¼ é‡æˆ–å•ä¸ªå¼ é‡ï¼Œå…¶æ¢¯åº¦å°†å½’ä¸€åŒ–
  - `max_norm`ï¼šæ¢¯åº¦çš„æœ€å¤§èŒƒæ•°
  - `norm_type`ï¼šfloatï¼Œé»˜è®¤ä¸º2.0ï¼Œç”¨çš„ p-èŒƒæ•°çš„ç±»å‹ã€‚infè¡¨ç¤ºæ— ç©·èŒƒæ•°ã€‚

```python
from accelerate import Accelerator

accelerator = Accelerator(gradient_accumulation_steps=2)
dataloader, model, optimizer, scheduler = accelerator.prepare(
dataloader, model, optimizer, scheduler
)

for input, target in dataloader:
     optimizer.zero_grad()
     output = model(input)
     loss = loss_func(output, target)
     accelerator.backward(loss)
     if accelerator.sync_gradients:
     # äºŒè€…å–å…¶ä¸€ï¼š
    	accelerator.clip_grad_value_(model.parameters(), clip_value)
        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)
        
     optimizer.step()

```

### æ··åˆç²¾åº¦è®­ç»ƒ
æ··åˆç²¾åº¦é€šè¿‡ä½¿ç”¨ fp16ï¼ˆåŠç²¾åº¦ï¼‰ç­‰è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚è¦æƒ³ä½¿ç”¨ Accelerate è·å¾—æœ€ä½³æ€§èƒ½ï¼Œåº”åœ¨æ¨¡å‹å†…éƒ¨è®¡ç®—æŸå¤±ï¼ˆå¦‚åœ¨ Transformers æ¨¡å‹ä¸­ï¼‰ï¼Œå› ä¸ºæ¨¡å‹å¤–éƒ¨çš„è®¡ç®—æ˜¯ä»¥å…¨ç²¾åº¦è¿›è¡Œçš„ã€‚

è®¾ç½®è¦åœ¨ `accelerater` ä¸­ä½¿ç”¨çš„æ··åˆç²¾åº¦ç±»å‹ï¼Œç„¶åä½¿ç”¨ `autocast()` ä¸Šä¸‹æ–‡ç®¡ç†å™¨å°†å€¼è‡ªåŠ¨è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ã€‚

```python
from accelerate import Accelerator
+ accelerator = Accelerator(mixed_precision="fp16")

+ with accelerator.autocast():
      loss = complex_loss_function(outputs, target):

```

## ä¿å­˜å’ŒåŠ è½½
è®­ç»ƒå®Œæˆåï¼ŒåŠ é€Ÿè¿˜å¯ä»¥ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œæˆ–è€…æ‚¨è¿˜å¯ä»¥ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆoptimizer stateï¼‰ï¼Œè¿™å¯¹äºæ¢å¤è®­ç»ƒå¾ˆæœ‰ç”¨ã€‚

### æ¨¡å‹
æ‰€æœ‰è¿‡ç¨‹å®Œæˆåï¼Œåœ¨ä¿å­˜æ¨¡å‹å‰ä½¿ç”¨ `unwrap_model()` æ–¹æ³•è§£é™¤æ¨¡å‹çš„å°è£…ï¼Œå› ä¸ºè®­ç»ƒå¼€å§‹å‰æ‰§è¡Œçš„ `prepare()` æ–¹æ³•å°†æ¨¡å‹å°è£…åˆ°äº†é€‚åˆçš„åˆ†å¸ƒå¼è®­ç»ƒæ¥å£ä¸­ã€‚å¦‚æœä¸è§£é™¤å¯¹æ¨¡å‹çš„å°è£…ï¼Œä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸çš„åŒæ—¶ä¹Ÿä¼šä¿å­˜å¤§æ¨¡å‹ä¸­ä»»ä½•æ½œåœ¨çš„é¢å¤–å±‚ï¼Œè¿™æ ·å°±æ— æ³•å°†æƒé‡åŠ è½½å›åŸºç¡€æ¨¡å‹ä¸­ã€‚

ä½¿ç”¨ `save_model()` æ–¹æ³•æ¥è§£åŒ…å¹¶ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚æ­¤æ–¹æ³•è¿˜å¯ä»¥å°†æ¨¡å‹ä¿å­˜åˆ°åˆ‡ç‰‡æ£€æŸ¥ç‚¹ `sharded checkpoints` æˆ–`safetensors`æ ¼å¼ä¸­ã€‚

```python
accelerator.wait_for_everyone()
accelerator.save_model(model, save_directory)
```

å¯¹äº `Transformers` åº“ä¸­çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ `save_pretrained` æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•é‡æ–°åŠ è½½ã€‚

```python
from transformers import AutoModel

unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(
    "path/to/my_model_directory",
    is_main_process=accelerator.is_main_process,
    save_function=accelerator.save,
)

model = AutoModel.from_pretrained("path/to/my_model_directory")
```

è¦åŠ è½½æƒé‡ï¼Œè¯·åœ¨åŠ è½½æƒé‡ä¹‹å‰å…ˆä½¿ç”¨ `unwrap_model()` æ–¹æ³•è§£åŒ…æ¨¡å‹ã€‚æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½æ˜¯å¯¹å¼ é‡çš„å¼•ç”¨ï¼Œå› æ­¤è¿™ä¼šå°†æ‚¨çš„æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚
```python
unwrapped_model = accelerator.unwrap_model(model)
path_to_checkpoint = os.path.join(save_directory,"pytorch_model.bin")
unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))
```

#### åˆ‡ç‰‡æ£€æŸ¥ç‚¹
è®¾ç½® `safe_serialization=True` å°†æ¨¡å‹ä¿å­˜ä¸º `safetensor` æ ¼å¼ã€‚

```python
accelerator.wait_for_everyone()
accelerator.save_model(model, save_directory, max_shard_size="1GB", safe_serialization=True)
```

è¦åŠ è½½åˆ†ç‰‡æ£€æŸ¥ç‚¹æˆ– `safetensor` æ ¼å¼çš„æ£€æŸ¥ç‚¹ï¼Œè¯·ä½¿ç”¨ `load_checkpoint_in_model()` æ–¹æ³•ã€‚æ­¤æ–¹æ³•å…è®¸æ‚¨å°†æ£€æŸ¥ç‚¹åŠ è½½åˆ°ç‰¹å®šè®¾å¤‡ä¸Šã€‚
```python
load_checkpoint_in_model(unwrapped_model, save_directory, device_map={"":device})
```

### çŠ¶æ€
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€éšæœºç”Ÿæˆå™¨ä»¥åŠå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å½“å‰çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨åŒä¸€ä¸ªè„šæœ¬ä¸­æ¢å¤å®ƒä»¬ã€‚ä½ åº”è¯¥åœ¨è„šæœ¬ä¸­æ·»åŠ  `save_state()` å’Œ `load_state()` æ–¹æ³•æ¥ä¿å­˜å’ŒåŠ è½½çŠ¶æ€ã€‚

ä»»ä½•å…¶ä»–éœ€è¦å­˜å‚¨çš„æœ‰çŠ¶æ€é¡¹ç›®éƒ½åº”ä½¿ç”¨ `register_for_checkpointing()` æ–¹æ³•æ³¨å†Œï¼Œä»¥ä¾¿ä¿å­˜å’ŒåŠ è½½ã€‚ä¼ é€’ç»™æ­¤æ–¹æ³•çš„æ¯ä¸ªè¦å­˜å‚¨çš„å¯¹è±¡éƒ½å¿…é¡»å…·æœ‰ `load_state_dict` å’Œ `state_dict` å‡½æ•°ã€‚

## æ‰§è¡Œè¿›ç¨‹
åœ¨ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ—¶ï¼Œç®¡ç†è·¨ `GPU` æ‰§è¡Œæµç¨‹çš„æ–¹å¼å’Œæ—¶é—´éå¸¸é‡è¦ã€‚æœ‰äº›è¿›ç¨‹æ¯”å…¶ä»–è¿›ç¨‹å®Œæˆå¾—æ›´å¿«ï¼Œæœ‰äº›è¿›ç¨‹åœ¨å…¶ä»–è¿›ç¨‹å°šæœªå®Œæˆæ—¶å°±ä¸åº”å¼€å§‹ã€‚Accelerate æä¾›äº†ç”¨äºåè°ƒè¿›ç¨‹æ‰§è¡Œæ—¶é—´çš„å·¥å…·ï¼Œä»¥ç¡®ä¿æ‰€æœ‰è®¾å¤‡ä¸Šçš„ä¸€åˆ‡ä¿æŒåŒæ­¥ã€‚

### åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šæ‰§è¡Œ
æŸäº›ä»£ç åªéœ€åœ¨ç‰¹å®šæœºå™¨ä¸Šè¿è¡Œä¸€æ¬¡ï¼Œå¦‚æ‰“å°æ—¥å¿—è¯­å¥æˆ–åªåœ¨æœ¬åœ°ä¸»è¿›ç¨‹ä¸Šæ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ¡ã€‚

#### statement
åº”ä½¿ç”¨ `accelerator.is_local_main_process` æ¥æŒ‡ç¤ºåªåº”æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ã€‚

  `accelerator.is_local_main_process` ï¼š
  - ç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æœ¬åœ°èŠ‚ç‚¹ï¼ˆæœåŠ¡å™¨ï¼‰ä¸Šçš„ä¸»è¿›ç¨‹ï¼Œ
  - å¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œæ¯å°æœåŠ¡å™¨éƒ½æœ‰ä¸€ä¸ªä¸»è¿›ç¨‹ã€‚`is_local_main_process()` å¦‚æœè¿”å› `True`ï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹æ˜¯æœ¬åœ°èŠ‚ç‚¹ä¸Šçš„ä¸»è¿›ç¨‹ã€‚
  - é€šå¸¸ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°èŠ‚ç‚¹çš„ä¸»è¿›ç¨‹ä¸Šæ‰§è¡Œä¸€äº›åªéœ€æ‰§è¡Œä¸€æ¬¡çš„æ“ä½œï¼Œä¾‹å¦‚åˆå§‹åŒ–æ•°æ®ã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç­‰ã€‚

```python
from tqdm.auto import tqdm

progress_bar = tqdm(
    range(args.max_train_steps), 
    disable=not accelerator.is_local_main_process
)
```

è¿˜å¯ä»¥ä½¿ç”¨ `accelerator.is_local_main_process` åŒ…è£…è¯­å¥ã€‚
```python
if accelerator.is_local_main_process:
    print("Accelerate is the best")
```

è¿˜å¯ä»¥æŒ‡ç¤º Accelerate åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­éƒ½è¦æ‰§è¡Œä¸€æ¬¡çš„ä»£ç ï¼Œè€Œä¸ç®¡æœ‰å¤šå°‘å°æœºå™¨ã€‚å¦‚æœæ‚¨è¦å°†æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

  `accelerator.is_main_process`ï¼š

  - è¿™ä¸ªå‡½æ•°ç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æ•´ä¸ªè®­ç»ƒä»»åŠ¡ä¸­çš„ä¸»è¿›ç¨‹ã€‚
  - ä¸»è¿›ç¨‹é€šå¸¸è´Ÿè´£ä¸€äº›å…¨å±€æ“ä½œï¼Œä¾‹å¦‚æ¨¡å‹ä¿å­˜ã€æ—¥å¿—è®°å½•ç­‰ã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨ `is_main_process()` æ¥ç¡®ä¿è¿™äº›æ“ä½œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡ã€‚
  - å¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡åœ¨å¤šå°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œ`is_main_process()` å°†è¿”å› `True`ï¼Œåªæœ‰ä¸€ä¸ªæœåŠ¡å™¨ä¸Šçš„ä¸»è¿›ç¨‹ä¼šæ»¡è¶³è¿™ä¸ªæ¡ä»¶ã€‚

```python
if accelerator.is_main_process:
    repo.push_to_hub()
```


#### function
å¯¹äºåªåº”æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ `on_local_main_process` è£…é¥°å™¨ã€‚
```python
@accelerator.on_local_main_process
def do_my_thing():
    "Something done once per server"
    do_thing_once_per_server()
```

å¯¹äºåªåº”åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡çš„å‡½æ•°ï¼Œè¯·ä½¿ç”¨ `on_main_process` è£…é¥°å™¨ã€‚
```python
@accelerator.on_main_process
def do_my_thing():
    "Something done once per server"
    do_thing_once()
```

### åœ¨ç‰¹å®šè¿›ç¨‹ä¸Šæ‰§è¡Œ

Accelerate è¿˜å¯ä»¥æ‰§è¡Œåªåº”åœ¨ç‰¹å®šè¿›ç¨‹æˆ–æœ¬åœ°è¿›ç¨‹ç´¢å¼•ä¸Šæ‰§è¡Œçš„å‡½æ•°ã€‚

ä½¿ç”¨ `on_process()` è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„è¿›ç¨‹ç´¢å¼•ã€‚

```python
@accelerator.on_process(process_index=0)
def do_my_thing():
    "Something done on process index 0"
    do_thing_on_index_zero()
```

ä½¿ç”¨ `on_local_process()` è£…é¥°å™¨æŒ‡å®šè¦æ‰§è¡Œå‡½æ•°çš„æœ¬åœ°è¿›ç¨‹ç´¢å¼•ã€‚
```python
@accelerator.on_local_process(local_process_idx=0)
def do_my_thing():
    "Something done on process index 0 on each server"
    do_thing_on_index_zero_on_each_server()
```

### æ¨è¿Ÿæ‰§è¡Œ
å½“åŒæ—¶åœ¨å¤šä¸ª `GPU` ä¸Šè¿è¡Œè„šæœ¬æ—¶ï¼ŒæŸäº›ä»£ç çš„æ‰§è¡Œé€Ÿåº¦å¯èƒ½ä¼šæ¯”å…¶ä»–ä»£ç å¿«ã€‚åœ¨æ‰§è¡Œä¸‹ä¸€ç»„æŒ‡ä»¤ä¹‹å‰ï¼Œæ‚¨å¯èƒ½éœ€è¦ç­‰å¾…æ‰€æœ‰è¿›ç¨‹éƒ½è¾¾åˆ°ä¸€å®šç¨‹åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½å®Œæˆè®­ç»ƒä¹‹å‰ï¼Œæ‚¨ä¸åº”è¯¥ä¿å­˜æ¨¡å‹ã€‚

ä¸ºæ­¤ï¼Œè¯·åœ¨ä»£ç ä¸­æ·»åŠ  `wait_for_everyone()`ã€‚è¿™ä¼šé˜»æ­¢æ‰€æœ‰å…ˆå®Œæˆè®­ç»ƒçš„è¿›ç¨‹ç»§ç»­è®­ç»ƒï¼Œç›´åˆ°æ‰€æœ‰å‰©ä½™è¿›ç¨‹éƒ½è¾¾åˆ°ç›¸åŒç‚¹ï¼ˆå¦‚æœåœ¨å•ä¸ª `GPU` æˆ– `CPU` ä¸Šè¿è¡Œï¼Œåˆ™æ²¡æœ‰å½±å“ï¼‰ã€‚

```python
accelerator.wait_for_everyone()
```

## å¯åŠ¨Accelerateè„šæœ¬
é¦–å…ˆï¼Œå°†è®­ç»ƒä»£ç é‡å†™ä¸ºå‡½æ•°ï¼Œå¹¶ä½¿å…¶å¯ä½œä¸ºè„šæœ¬è°ƒç”¨ã€‚ä¾‹å¦‚ï¼š
```python
  from accelerate import Accelerator
  
+ def main():
      accelerator = Accelerator()

      model, optimizer, training_dataloader, scheduler = accelerator.prepare(
          model, optimizer, training_dataloader, scheduler
      )

      for batch in training_dataloader:
          optimizer.zero_grad()
          inputs, targets = batch
          outputs = model(inputs)
          loss = loss_function(outputs, targets)
          accelerator.backward(loss)
          optimizer.step()
          scheduler.step()

+ if __name__ == "__main__":
+     main()

```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼š
```shell
accelerate launch --accelerate-arg {script_name.py} --script-arg1 --script-arg2 ...
```

ä½¿ç”¨å•ä¸ª `GPU` ï¼š
```shell
CUDA_VISIBLE_DEVICES="0" accelerate launch {script_name.py} --arg1 --arg2 ...
```

æŒ‡å®šè¦ä½¿ç”¨çš„ `GPU` æ•°é‡ï¼š

```shell
accelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...
```

ä½¿ç”¨æ··åˆç²¾åº¦åœ¨ä¸¤ä¸ª `GPU` ä¸Šå¯åŠ¨ç›¸åŒçš„è„šæœ¬:
```shell
accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...
```

è¦è·å–å¯ä»¥ä¼ å…¥çš„å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·è¿è¡Œï¼š
```shell
accelerate launch -h
```

ä»è¯¥è‡ªå®šä¹‰ `yaml` æ–‡ä»¶å¯åŠ¨è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
accelerate launch --config_file {path/to/config/my_config_file.yaml} {script_name.py} {--arg1} {--arg2} ...
```
