# å¤§æ¨¡å‹è®­ç»ƒå·¥å…·ä¹‹Accelerate

# accelerateåŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒ

éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå¹¶è¡Œæ€§å·²ç»æˆä¸ºåœ¨æœ‰é™ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§æ¨¡å‹å’ŒåŠ é€Ÿè®­ç»ƒé€Ÿåº¦çš„ç­–ç•¥ï¼Œå¢åŠ äº†æ•°ä¸ªæ•°é‡çº§ã€‚Hugging Faceï¼Œæä¾›äº†ğŸ¤— [åŠ é€Ÿåº“](https://huggingface.co/docs/accelerate)ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè½»æ¾è®­ç»ƒğŸ¤— Transformersæ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šçš„å¤šä¸ªGPUè¿˜æ˜¯åœ¨å¤šä¸ªæœºå™¨ä¸Šçš„å¤šä¸ªGPUã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œäº†è§£å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„åŸç”ŸPyTorchè®­ç»ƒå¾ªç¯ï¼Œä»¥å¯ç”¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è®­ç»ƒã€‚

## è®¾ç½®
é€šè¿‡å®‰è£…ğŸ¤— åŠ é€Ÿå¼€å§‹:

```shell
pip install accelerate
```

ç„¶åå¯¼å…¥å¹¶åˆ›å»º`Accelerator`å¯¹è±¡ã€‚`Accelerator`å°†è‡ªåŠ¨æ£€æµ‹æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ç±»å‹ï¼Œå¹¶åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶ã€‚æ‚¨ä¸éœ€è¦æ˜¾å¼åœ°å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šã€‚

```python
from accelerate import Accelerator

accelerator = Accelerator()
```

## å‡†å¤‡åŠ é€Ÿ
ä¸‹ä¸€æ­¥æ˜¯å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™`prepare`æ–¹æ³•ã€‚è¿™åŒ…æ‹¬æ‚¨çš„è®­ç»ƒå’Œè¯„ä¼°DataLoaderã€ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªä¼˜åŒ–å™¨:

```python
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

## åå‘ä¼ æ’­
æœ€åä¸€æ­¥æ˜¯ç”¨ğŸ¤— åŠ é€Ÿçš„`backward`æ–¹æ³•æ›¿æ¢è®­ç»ƒå¾ªç¯ä¸­çš„å…¸å‹`loss.backward()`:

```python
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

å¦‚æ‚¨åœ¨ä¸‹é¢çš„ä»£ç ä¸­æ‰€è§ï¼Œæ‚¨åªéœ€è¦æ·»åŠ å››è¡Œé¢å¤–çš„ä»£ç åˆ°æ‚¨çš„è®­ç»ƒå¾ªç¯ä¸­å³å¯å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼

```python
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## å¯åŠ¨è®­ç»ƒ
è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå’Œä¿å­˜é…ç½®æ–‡ä»¶:
```shell
accelerate config
```

ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ:
```shell
accelerate launch train.py
```
